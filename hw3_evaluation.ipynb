{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from Hugging Face\n",
    "sentiment_dataset = load_dataset('carblacac/twitter-sentiment-analysis')\n",
    "\n",
    "# Access the training and testing sets directly\n",
    "train_dataset = sentiment_dataset['train']\n",
    "test_dataset = sentiment_dataset['test']\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "train_df = train_dataset.to_pandas()\n",
    "test_df = test_dataset.to_pandas()\n",
    "\n",
    "# Select the first 1000 rows of the train dataset\n",
    "train_df = train_df[:1000] #30000\n",
    "\n",
    "# Select the first 50 rows of the test dataset\n",
    "# test_df = test_dataset.to_pandas()[:50]\n",
    "test_df = test_df[:50]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define two detailed sets of instructions\n",
    "instructions1 = \"\"\"\n",
    "Please read the tweet provided below. Your task is to analyze the content and context of the tweet to determine whether it \n",
    "expresses a positive or negative sentiment. Consider the use of emotive language, punctuation, and any emoticons used. \n",
    "Classify the tweet accordingly as either '1' if it conveys a favorable opinion or emotion, or '0' if it \n",
    "expresses an unfavorable opinion or emotion.\n",
    "### Tweet: {tweet}\n",
    "### Sentiment\n",
    "\"\"\"\n",
    "\n",
    "# Function to append instructions to each row\n",
    "def add_instructions(df):\n",
    "    df['Instruction1'] = df.apply(lambda x: instructions1.format(tweet=x['text'], sentiment=x['feeling']), axis=1)\n",
    "    return df\n",
    "\n",
    "# Apply instructions to the full test datasets\n",
    "test_df = add_instructions(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please read the tweet provided below. Your task is to analyze the content and context of the tweet to determine whether it \n",
      "expresses a positive or negative sentiment. Consider the use of emotive language, punctuation, and any emoticons used. \n",
      "Classify the tweet accordingly as either '1' if it conveys a favorable opinion or emotion, or '0' if it \n",
      "expresses an unfavorable opinion or emotion.\n",
      "### Tweet: @justineville ...yeahhh. ) i'm 39 tweets from 1,600!\n",
      "### Sentiment\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_df.iloc[0]['Instruction1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Function to set up and run the pipeline\n",
    "def run_model(model, tokenizer, test_dset, file_name):\n",
    "\n",
    "    # Create the pipeline\n",
    "    pipe = pipeline(\n",
    "        task=\"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        device=0\n",
    "    )\n",
    "\n",
    "    # Generate responses\n",
    "    batch_size = 20\n",
    "    num_examples = len(test_dset)\n",
    "    total_batches = (num_examples + batch_size - 1) // batch_size\n",
    "    generated_output = []\n",
    "\n",
    "    for i in tqdm(range(0, num_examples, batch_size), total=total_batches, desc=\"Generating text\"):\n",
    "        batch_indices = range(i, min(i + batch_size, num_examples))\n",
    "        batch = test_dset.select(batch_indices)\n",
    "        prompts = [example['Instruction1'].split('### Sentiment:')[0].strip() for example in batch]\n",
    "        results = pipe(prompts, max_new_tokens=64)\n",
    "        \n",
    "        # counter=0\n",
    "        for result in results:\n",
    "            generated_text = result[0]['generated_text']\n",
    "            # generated_output.append(generated_text)\n",
    "            prompt = prompts[results.index(result)]\n",
    "            reference_answer = batch[results.index(result)]['feeling']\n",
    "            \n",
    "            # counter += 1\n",
    "            # print(counter)\n",
    "            # print(f\"Prompt: {prompt}\\n\")\n",
    "            # print(f\"Reference Answer: {reference_answer}\\n\")\n",
    "            # print(f\"Generated Text: {generated_text}\")\n",
    "            # print(\"------\")\n",
    "\n",
    "            # Save results in structured format\n",
    "            generated_output.append({\n",
    "                \"Prompt\": prompt,\n",
    "                \"Reference_answer\": reference_answer,\n",
    "                \"Generated_outputs\": generated_text\n",
    "            })\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(f'/work/gns938/nlp_hw3/{file_name}.json', 'w') as f:\n",
    "        json.dump(generated_output, f, indent=4)\n",
    "\n",
    "    print(f\"Results saved to '{file_name}.json'.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.36s/it]\n",
      "Generating text: 100%|██████████| 3/3 [02:02<00:00, 40.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 'generated_output_sentiment_fine_tuned.json'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.95s/it]\n",
      "Generating text: 100%|██████████| 3/3 [02:02<00:00, 40.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 'generated_output_mixed_fine_tuned.json'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.87s/it]\n",
      "Generating text:   0%|          | 0/3 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating text:  33%|███▎      | 1/3 [00:46<01:33, 46.78s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating text:  67%|██████▋   | 2/3 [01:33<00:46, 46.55s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Generating text: 100%|██████████| 3/3 [01:56<00:00, 38.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 'generated_output_original_pretrained.json'.\n"
     ]
    }
   ],
   "source": [
    "model_paths = [\n",
    "    '/work/gns938/nlp_hw3/Mistral-sentiment-fine-tuned',\n",
    "    '/work/gns938/nlp_hw3/Mistral-mixed-fine-tuned',\n",
    "    'mistralai/Mistral-7B-v0.1'\n",
    "]\n",
    "file_names = ['generated_output_sentiment_fine_tuned', 'generated_output_mixed_fine_tuned', 'generated_output_original_pretrained']\n",
    "\n",
    "for model_path, file_name in zip(model_paths, file_names):\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    run_model(model, tokenizer, test_dset, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for generated_output_sentiment_fine_tuned.json:\n",
      "Accuracy: 0.92\n",
      "F1 Score: 0.9130434782608695\n",
      "Precision: 0.9130434782608695\n",
      "Recall: 0.9130434782608695\n",
      "\n",
      "Results for generated_output_mixed_fine_tuned.json:\n",
      "Accuracy: 0.88\n",
      "F1 Score: 0.8636363636363636\n",
      "Precision: 0.9047619047619048\n",
      "Recall: 0.8260869565217391\n",
      "\n",
      "Results for generated_output_original_pretrained.json:\n",
      "Accuracy: 0.62\n",
      "F1 Score: 0.6885245901639344\n",
      "Precision: 0.5526315789473685\n",
      "Recall: 0.9130434782608695\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import os\n",
    "\n",
    "# Function to extract sentiment from generated outputs\n",
    "def extract_sentiment(generated_text):\n",
    "    try:\n",
    "        # Split the text and extract the part after \"### Sentiment: \"\n",
    "        sentiment_part = generated_text.split(\"\\n### Sentiment: \")[1]\n",
    "        # Now extract the sentiment value before any subsequent newline\n",
    "        sentiment = sentiment_part.split(\"\\n\")[0].strip()\n",
    "        return int(sentiment)  # Ensure it's converted to integer\n",
    "    except (IndexError, ValueError):\n",
    "        # In case of parsing or conversion error, return None\n",
    "        return None\n",
    "\n",
    "def evaluate_results(file_path):\n",
    "    # Load the results from JSON file\n",
    "    with open(file_path, 'r') as f:\n",
    "        results = json.load(f)\n",
    "\n",
    "    # Extract reference and predicted sentiments\n",
    "    reference_answers = [int(result[\"Reference_answer\"]) for result in results]\n",
    "    predicted_sentiments = [extract_sentiment(result[\"Generated_outputs\"]) for result in results]\n",
    "\n",
    "    # Remove None values and ensure all items are integers\n",
    "    filtered_reference_answers = []\n",
    "    filtered_predicted_sentiments = []\n",
    "    for ref, pred in zip(reference_answers, predicted_sentiments):\n",
    "        if pred is not None:\n",
    "            filtered_reference_answers.append(ref)\n",
    "            filtered_predicted_sentiments.append(pred)\n",
    "\n",
    "    # Calculate accuracy, F1 score, precision, and recall\n",
    "    accuracy = accuracy_score(filtered_reference_answers, filtered_predicted_sentiments)\n",
    "    f1 = f1_score(filtered_reference_answers, filtered_predicted_sentiments, average='binary')\n",
    "    precision = precision_score(filtered_reference_answers, filtered_predicted_sentiments, average='binary')\n",
    "    recall = recall_score(filtered_reference_answers, filtered_predicted_sentiments, average='binary')\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Results for {os.path.basename(file_path)}:\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\\n\")\n",
    "\n",
    "# List of JSON files to evaluate\n",
    "json_files = [\n",
    "    '/work/gns938/nlp_hw3/generated_output_sentiment_fine_tuned.json',\n",
    "    '/work/gns938/nlp_hw3/generated_output_mixed_fine_tuned.json',\n",
    "    '/work/gns938/nlp_hw3/generated_output_original_pretrained.json'\n",
    "]\n",
    "\n",
    "# Evaluate each file\n",
    "for file_path in json_files:\n",
    "    evaluate_results(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.21s/it]\n",
      "Generating text: 100%|██████████| 10/10 [00:47<00:00,  4.74s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.04s/it]\n",
      "Generating text: 100%|██████████| 10/10 [00:42<00:00,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to 'generated_output_instructions.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "model_sentiment_path = '/work/gns938/nlp_hw3/Mistral-sentiment-fine-tuned'\n",
    "model_mixed_path = '/work/gns938/nlp_hw3/Mistral-mixed-fine-tuned'\n",
    "\n",
    "instructions = [\n",
    "    \"### Instruction:\\nSummarize the text provided in one sentence.\\n\\n### Input:\\nArtificial intelligence refers to the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions.\\n\\n### Answer: \",\n",
    "    \"### Instruction:\\nTranslate the following sentence from English to French.\\n\\n### Input: What time is the sunset tonight?\\n\\n### Answer: \",\n",
    "    \"### Instruction:\\nGenerate a question based on the text.\\n\\n### Input: Global warming is causing Arctic ice to melt at unprecedented rates.\\n\\n### Answer: \",\n",
    "    \"### Instruction:\\nExplain the implications of this historical event.\\n\\n### Input: In 1990, Germany was reunified, marking the end of the Cold War in Europe.\\n\\n### Answer: \",\n",
    "    \"### Instruction:\\nWrite a poem about the theme described.\\n\\n### Input: The relentless pursuit of technological advancement.\\n\\n### Answer: \",\n",
    "    \"### Instruction:\\nDescribe the steps involved in photosynthesis.\\n\\n### Input: Photosynthesis is the process by which green plants and some other organisms use sunlight to synthesize foods from carbon dioxide and water.\\n### Answer: \",\n",
    "    \"### Instruction:\\nConvert this dialogue into a formal report.\\n\\n### Input: Customer: 'I'd like to return this item.' Sales Assistant: 'Certainly, do you have the receipt?'\\n\\n### Answer: \",\n",
    "    \"### Instruction:\\nCreate a list of recommendations based on the user's preferences.\\n\\n### Input: The user enjoys historical novels, prefers complex characters, and dislikes predictable plots.\\n\\n### Answer: \",\n",
    "    \"### Instruction:\\nDeduce the moral of the story provided.\\n\\n### Input: A fox, after failing to reach a bunch of grapes, declared they were sour anyway.\\n\\n### Answer: \",\n",
    "    \"### Instruction:\\nIdentify and explain the literary devices used in this text.\\n\\n### Input: The wind whispered through the dark, foreboding woods.\\n\\n### Answer: \"\n",
    "]\n",
    "\n",
    "test_dataset = Dataset.from_dict({'text': instructions})\n",
    "\n",
    "def generate_response(model_path, test_dataset):\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "    \n",
    "    batch_size = 1\n",
    "    num_examples = len(test_dataset)\n",
    "    total_batches = (num_examples + batch_size - 1) // batch_size\n",
    "    generated_output = []\n",
    "\n",
    "    for i in tqdm(range(0, num_examples, batch_size), total=total_batches, desc=\"Generating text\"):\n",
    "        batch_indices = range(i, min(i + batch_size, num_examples))\n",
    "        batch = test_dataset.select(batch_indices)\n",
    "        prompts = [example['text'] for example in batch]\n",
    "\n",
    "        results = pipe(prompts, max_new_tokens=128)\n",
    "        \n",
    "        for result in results:\n",
    "            generated_text = result[0]['generated_text']\n",
    "            generated_output.append(generated_text)\n",
    "\n",
    "    return generated_output\n",
    "\n",
    "responses_sentiment = generate_response(model_sentiment_path, test_dataset)\n",
    "responses_mixed = generate_response(model_mixed_path, test_dataset)\n",
    "\n",
    "results_json = []\n",
    "for i, instruction in enumerate(instructions):\n",
    "    results_json.append({\n",
    "        \"Instruction\": instruction,\n",
    "        \"Sentiment_Fine_Tuned\": responses_sentiment[i],\n",
    "        \"Mixed_Fine_Tuned\": responses_mixed[i]\n",
    "    })\n",
    "\n",
    "with open('/work/gns938/nlp_hw3/generated_output_instructions.json', 'w+') as f:\n",
    "    json.dump(results_json, f, indent=4)\n",
    "\n",
    "print(\"Results saved to 'generated_output_instructions.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
